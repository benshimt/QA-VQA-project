datasets
datasets
datasets
datasets
datasets
datasets
datasets
datasets
datasets
datasets
finish imports
finish parsing

Namespace(model_parallel=True, model_name='opt-2.7b', seed=4, prompt_text='', max_length=128, batch_size=32, epoch=8, lr=6e-06, time_stamp='2023-11-28_16-07-25', test_only='False', scale=1187, model_path='', loss_rate=1.0, token_loss=0)


************************************************************
start train...
************************************************************

The time stamp is 2023-11-28_16-07-25
load data start.
load data done.
data process start.
Map:   0%|                                                                                             | 0/2374 [00:00<?, ? examples/s]Map:  42%|█████████████████████████████████▋                                              | 1000/2374 [00:00<00:00, 3140.73 examples/s]Map:  84%|███████████████████████████████████████████████████████████████████▍            | 2000/2374 [00:00<00:00, 3622.50 examples/s]Map: 100%|████████████████████████████████████████████████████████████████████████████████| 2374/2374 [00:00<00:00, 3608.31 examples/s]Map: 100%|████████████████████████████████████████████████████████████████████████████████| 2374/2374 [00:00<00:00, 3489.03 examples/s]
Map:   0%|                                                                                              | 0/982 [00:00<?, ? examples/s]Map: 100%|██████████████████████████████████████████████████████████████████████████████████| 982/982 [00:00<00:00, 3883.66 examples/s]Map: 100%|██████████████████████████████████████████████████████████████████████████████████| 982/982 [00:00<00:00, 3688.30 examples/s]
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
data process done.
model load start.
cuda
model load done.
set training arguments start.
set training arguments done.
data collator load start.
data collator load done.
trainer load start.
trainer load done.
model training start.
  0%|                                                                                                          | 0/600 [00:00<?, ?it/s]  0%|▏                                                                                                 | 1/600 [00:03<33:20,  3.34s/it]Traceback (most recent call last):
  File "/sise/eliorsu-group/Project-11-2023/FalseQA/src/exp-3_opt.py", line 406, in <module>
    train()
  File "/sise/eliorsu-group/Project-11-2023/FalseQA/src/exp-3_opt.py", line 297, in train
    trainer.train()
  File "/home/ronfay/.conda/envs/project/lib/python3.10/site-packages/transformers/trainer.py", line 1555, in train
    return inner_training_loop(
  File "/home/ronfay/.conda/envs/project/lib/python3.10/site-packages/transformers/trainer.py", line 1837, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ronfay/.conda/envs/project/lib/python3.10/site-packages/transformers/trainer.py", line 2682, in training_step
    loss = self.compute_loss(model, inputs)
  File "/sise/eliorsu-group/Project-11-2023/FalseQA/src/exp-3_opt.py", line 56, in compute_loss
    outputs = model(**inputs)
  File "/home/ronfay/.conda/envs/project/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ronfay/.conda/envs/project/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 944, in forward
    outputs = self.model.decoder(
  File "/home/ronfay/.conda/envs/project/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ronfay/.conda/envs/project/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 710, in forward
    layer_outputs = decoder_layer(
  File "/home/ronfay/.conda/envs/project/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ronfay/.conda/envs/project/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 353, in forward
    hidden_states = self.fc1(hidden_states)
  File "/home/ronfay/.conda/envs/project/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ronfay/.conda/envs/project/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 47.51 GiB total capacity; 42.62 GiB already allocated; 111.44 MiB free; 46.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  0%|▏                                                                                                 | 1/600 [00:03<38:30,  3.86s/it]
