{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instalations"
      ],
      "metadata": {
        "id": "VCT3YX9zINpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade typing-extensions\n",
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n5jpI9E3116",
        "outputId": "6968367a-061c-4d4c-af4b-ceb56ecc441b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (4.12.2)\n",
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.6.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT APIs"
      ],
      "metadata": {
        "id": "avOx3w8hIQl9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQP15g2ozsGM"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import time\n",
        "import os\n",
        "\n",
        "def _ms_since_epoch():\n",
        "    return time.perf_counter_ns() // 1000000\n",
        "\n",
        "\n",
        "def set_openai_parameters(engine, max_tokens):\n",
        "    # openai API setup and parameters\n",
        "    openai.api_key = \"sk-proj-LEu3lWTFbZ1inVRqd5E9T3BlbkFJqATqVJMZbDaGnm7nznzH\"\n",
        "    parameters = {\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"top_p\": 0,  # greedy\n",
        "        \"temperature\": 0.5,\n",
        "        \"logprobs\": 5,  # maximal value accorrding to https://beta.openai.com/docs/api-reference/completions/create#completions/create-logprobs, used to be 10...\n",
        "        \"engine\": engine,\n",
        "    }\n",
        "    time_of_last_api_call = _ms_since_epoch()\n",
        "\n",
        "    return parameters, time_of_last_api_call\n",
        "\n",
        "\n",
        "def wait_between_predictions(time_of_last_api_call, min_ms_between_api_calls):\n",
        "    if (\n",
        "        cur_time := _ms_since_epoch()\n",
        "    ) <= time_of_last_api_call + min_ms_between_api_calls:\n",
        "        ms_to_sleep = min_ms_between_api_calls - (cur_time - time_of_last_api_call)\n",
        "        time.sleep(ms_to_sleep / 1000)\n",
        "    time_of_last_api_call = _ms_since_epoch()\n",
        "\n",
        "\n",
        "def predict_sample_openai_gpt(\n",
        "    example,\n",
        "    prompt,\n",
        "    min_ms_between_api_calls: int = 5000,\n",
        "    engine: str = \"text-davinci-003\",\n",
        "    max_tokens: int = 100,\n",
        "):\n",
        "    parameters, time_of_last_api_call = set_openai_parameters(engine, max_tokens)\n",
        "    parameters[\"prompt\"] = prompt\n",
        "\n",
        "    # OpenAI limits us to 3000 calls per minute:\n",
        "    # https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\n",
        "    # that is why the default value of min_ms_between_api_calls is 20\n",
        "    wait_between_predictions(time_of_last_api_call, min_ms_between_api_calls)\n",
        "\n",
        "    response = openai.Completion.create(**parameters)\n",
        "\n",
        "    if response is None:\n",
        "        raise Exception(\"Response from OpenAI API is None.\")\n",
        "\n",
        "    # build output data\n",
        "    prediction = dict()\n",
        "    prediction[\"input\"] = prompt\n",
        "    prediction[\"prediction\"] = response.choices[0].text.strip().strip(\".\")  # type:ignore\n",
        "\n",
        "    # build output metadata\n",
        "    metadata = example.copy()  # dict()\n",
        "    metadata[\"logprobs\"] = response.choices[0][\"logprobs\"]  # type:ignore\n",
        "    # \"finish_reason\" is located in a slightly different location in opt\n",
        "    if \"opt\" in engine:\n",
        "        finish_reason = response.choices[0][\"logprobs\"][  # type:ignore\n",
        "            \"finish_reason\"\n",
        "        ]\n",
        "    else:\n",
        "        finish_reason = response.choices[0][\"finish_reason\"]  # type:ignore\n",
        "    metadata[\"finish_reason\"] = finish_reason\n",
        "    if \"opt\" not in engine:\n",
        "        # From the OpenAI API documentation it's not clear what \"index\" is, but let's keep it as well\n",
        "        metadata[\"index\"] = response.choices[0][\"index\"]  # type:ignore\n",
        "\n",
        "    prediction[\"metadata\"] = metadata\n",
        "\n",
        "    return prediction\n",
        "\n",
        "def predict_sample_openai_chatgpt(\n",
        "    prompt,\n",
        "    img_url,\n",
        "    min_ms_between_api_calls: int = 10000,\n",
        "    engine: str = \"gpt-4o\",\n",
        "    max_tokens: int = 100,\n",
        "):\n",
        "    parameters, time_of_last_api_call = set_openai_parameters(engine, max_tokens)\n",
        "    parameters[\"prompt\"] = prompt\n",
        "\n",
        "    # OpenAI limits us to 3000 calls per minute:\n",
        "    # https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\n",
        "    wait_time = 10\n",
        "    time.sleep(wait_time)\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(model=engine, messages=[{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\":prompt},{\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_url}\"\n",
        "}}]}], temperature=parameters['temperature'], top_p=parameters['top_p'])\n",
        "    except openai.error.RateLimitError as e:\n",
        "        wait_time = 10\n",
        "        print(f\"Rate limit reached. Waiting {wait_time} seconds.\")\n",
        "        time.sleep(wait_time)\n",
        "\n",
        "        response = openai.ChatCompletion.create(model=engine, messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                                                temperature=parameters['temperature'], top_p=parameters['top_p'])\n",
        "\n",
        "    if response is None:\n",
        "        raise Exception(\"Response from OpenAI API is None.\")\n",
        "\n",
        "    # build output data\n",
        "    prediction = dict()\n",
        "    prediction[\"input\"] = prompt\n",
        "    prediction[\"prediction\"] = response.choices[0].message['content']  # type:ignore\n",
        "\n",
        "    return prediction\n",
        "\n",
        "def gpt4_estimetion(url):\n",
        "  prompt= f\"\"\"\n",
        "Generate a caption for the provided image. If the image contains any nonsensical or uncommon elements, make sure to highlight them.\n",
        "  \"\"\"\n",
        "  gpt4_prediction = predict_sample_openai_chatgpt(prompt,url)\n",
        "  return gpt4_prediction['prediction']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prepare WHOOPS! dataset"
      ],
      "metadata": {
        "id": "VONpfPrkIT8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git-lfs\n",
        "!git clone https://huggingface.co/spaces/nlphuji/whoops-explorer-analysis\n",
        "!pip install -q datasets\n",
        "from datasets import load_dataset\n",
        "# import gradio as gr\n",
        "import os\n",
        "import random\n",
        "\n",
        "wmtis = load_dataset(\"nlphuji/wmtis-identify\")['test'][73:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSGd7fh11ff0",
        "outputId": "9725152f-463a-45ab-a5a8-26dbff6f9345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'whoops-explorer-analysis' already exists and is not an empty directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1491: FutureWarning: The repository for nlphuji/wmtis-identify contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/nlphuji/wmtis-identify\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run captions generation task on strange, normal, natural images"
      ],
      "metadata": {
        "id": "EP7D6uI3IYhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "from io import BytesIO\n",
        "from openai.error import InvalidRequestError\n",
        "\n",
        "gpt4_captions ={'natural_caption':[],'normal_caption':[],'strange_caption':[]}\n",
        "for record in wmtis:\n",
        "  print(record)\n",
        "  natural_image = record['natural_image']\n",
        "  normal_image = record['normal_image']\n",
        "  strange_image = record['strange_image']\n",
        "  print(record)\n",
        "\n",
        "  buffered = BytesIO()\n",
        "  natural_image.save(buffered, format=\"PNG\")\n",
        "  natural_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "  try:\n",
        "    natural_caption = gpt4_estimetion(natural_str)\n",
        "    print(natural_caption)\n",
        "    gpt4_captions['natural_caption'].append(natural_caption)\n",
        "  except InvalidRequestError as e:\n",
        "    gpt4_captions['natural_caption'].append(f'error: {natural_caption}')\n",
        "    print(f\"Failed to get caption: {e}\")\n",
        "\n",
        "\n",
        "  buffered = BytesIO()\n",
        "  normal_image.save(buffered, format=\"PNG\")\n",
        "  normal_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "  try:\n",
        "    normal_caption = gpt4_estimetion(normal_str)\n",
        "    print(normal_caption)\n",
        "    gpt4_captions['normal_caption'].append(normal_caption)\n",
        "  except InvalidRequestError as e:\n",
        "    gpt4_captions['normal_caption'].append(f'error: {normal_caption}')\n",
        "    print(f\"Failed to get caption: {e}\")\n",
        "\n",
        "  buffered = BytesIO()\n",
        "  strange_image.save(buffered, format=\"PNG\")\n",
        "  strange_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "  try:\n",
        "    strange_caption = gpt4_estimetion(strange_str)\n",
        "    print(strange_caption)\n",
        "    gpt4_captions['strange_caption'].append(strange_caption)\n",
        "  except InvalidRequestError as e:\n",
        "    gpt4_captions['strange_caption'].append(f'error: {strange_caption}')\n",
        "    print(f\"Failed to get caption: {e}\")\n"
      ],
      "metadata": {
        "id": "of-NCDt52-RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the outputs to a csv files\n",
        "import pandas as pd\n",
        "natural_df = pd.DataFrame(gpt4_captions['natural_caption'])\n",
        "natural_df.to_csv('natural_caption_improvedPrompt.csv', index=False)  # index=False to avoid writing row numbers\n",
        "\n",
        "normal_df = pd.DataFrame(gpt4_captions['normal_caption'])\n",
        "normal_df.to_csv('normal_caption_improvedPrompt.csv', index=False)  # index=False to avoid writing row numbers\n",
        "\n",
        "strange_df = pd.DataFrame(gpt4_captions['strange_caption'])\n",
        "strange_df.to_csv('strange_caption_improvedPrompt.csv', index=False)  # index=False to avoid writing row numbers"
      ],
      "metadata": {
        "id": "C4OXFVJYaRC9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}