{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCT3YX9zINpZ"
   },
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8n5jpI9E3116",
    "outputId": "6968367a-061c-4d4c-af4b-ceb56ecc441b"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade typing-extensions\n",
    "!pip install -q openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronfay/.conda/envs/blip2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "import openai\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from PIL import Image\n",
    "import requests\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import numpy as np\n",
    "import traceback\n",
    "from openai.error import InvalidRequestError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avOx3w8hIQl9"
   },
   "source": [
    "### GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uQP15g2ozsGM"
   },
   "outputs": [],
   "source": [
    "def _ms_since_epoch():\n",
    "    return time.perf_counter_ns() // 1000000\n",
    "\n",
    "\n",
    "def set_openai_parameters(engine, max_tokens):\n",
    "    # openai API setup and parameters\n",
    "    openai.api_key = \"key\"\n",
    "    parameters = {\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": 0,  # greedy\n",
    "        \"temperature\": 0.5,\n",
    "        \"logprobs\": 5, \n",
    "        \"engine\": engine,\n",
    "    }\n",
    "    time_of_last_api_call = _ms_since_epoch()\n",
    "\n",
    "    return parameters, time_of_last_api_call\n",
    "\n",
    "\n",
    "def wait_between_predictions(time_of_last_api_call, min_ms_between_api_calls):\n",
    "    if (\n",
    "        cur_time := _ms_since_epoch()\n",
    "    ) <= time_of_last_api_call + min_ms_between_api_calls:\n",
    "        ms_to_sleep = min_ms_between_api_calls - (cur_time - time_of_last_api_call)\n",
    "        time.sleep(ms_to_sleep / 1000)\n",
    "    time_of_last_api_call = _ms_since_epoch()\n",
    "\n",
    "\n",
    "def predict_sample_openai_gpt(\n",
    "    example,\n",
    "    prompt,\n",
    "    min_ms_between_api_calls: int = 5000,\n",
    "    engine: str = \"text-davinci-003\",\n",
    "    max_tokens: int = 100,\n",
    "):\n",
    "    parameters, time_of_last_api_call = set_openai_parameters(engine, max_tokens)\n",
    "    parameters[\"prompt\"] = prompt\n",
    "\n",
    "\n",
    "    wait_between_predictions(time_of_last_api_call, min_ms_between_api_calls)\n",
    "\n",
    "    response = openai.Completion.create(**parameters)\n",
    "\n",
    "    if response is None:\n",
    "        raise Exception(\"Response from OpenAI API is None.\")\n",
    "\n",
    "    # build output data\n",
    "    prediction = dict()\n",
    "    prediction[\"input\"] = prompt\n",
    "    prediction[\"prediction\"] = response.choices[0].text.strip().strip(\".\")  # type:ignore\n",
    "\n",
    "    # build output metadata\n",
    "    metadata = example.copy()  # dict()\n",
    "    metadata[\"logprobs\"] = response.choices[0][\"logprobs\"]  # type:ignore\n",
    "    # \"finish_reason\" is located in a slightly different location in opt\n",
    "    if \"opt\" in engine:\n",
    "        finish_reason = response.choices[0][\"logprobs\"][  # type:ignore\n",
    "            \"finish_reason\"\n",
    "        ]\n",
    "    else:\n",
    "        finish_reason = response.choices[0][\"finish_reason\"]  # type:ignore\n",
    "    metadata[\"finish_reason\"] = finish_reason\n",
    "    if \"opt\" not in engine:\n",
    "        # From the OpenAI API documentation it's not clear what \"index\" is, but let's keep it as well\n",
    "        metadata[\"index\"] = response.choices[0][\"index\"]  # type:ignore\n",
    "\n",
    "    prediction[\"metadata\"] = metadata\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def predict_sample_openai_chatgpt(\n",
    "    prompt,\n",
    "    img_url,\n",
    "    min_ms_between_api_calls: int = 10000,\n",
    "    engine: str = \"gpt-4o\",\n",
    "    max_tokens: int = 100,\n",
    "):\n",
    "    parameters, time_of_last_api_call = set_openai_parameters(engine, max_tokens)\n",
    "    parameters[\"prompt\"] = prompt\n",
    "\n",
    "    wait_time = 10\n",
    "    time.sleep(wait_time)\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(model=engine, messages=[{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\":prompt},{\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_url}\"\n",
    "}}]}], temperature=parameters['temperature'], top_p=parameters['top_p'])\n",
    "    except openai.error.RateLimitError as e:\n",
    "        wait_time = 10\n",
    "        print(f\"Rate limit reached. Waiting {wait_time} seconds.\")\n",
    "        time.sleep(wait_time)\n",
    "\n",
    "        response = openai.ChatCompletion.create(model=engine, messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                                                temperature=parameters['temperature'], top_p=parameters['top_p'])\n",
    "\n",
    "    if response is None:\n",
    "        raise Exception(\"Response from OpenAI API is None.\")\n",
    "\n",
    "    # build output data\n",
    "    prediction = dict()\n",
    "    prediction[\"input\"] = prompt\n",
    "    prediction[\"prediction\"] = response.choices[0].message['content']  # type:ignore\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def gpt4_estimetion(url):\n",
    "  # best prompt:\n",
    "  prompt= f\"\"\"\n",
    "Generate a caption for the provided image. If the image contains any nonsensical or uncommon elements, make sure to highlight them.\n",
    "  \"\"\"\n",
    "  gpt4_prediction = predict_sample_openai_chatgpt(prompt,url)\n",
    "  return gpt4_prediction['prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blip Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-vicuna-7b\", torch_dtype=torch.float16)\n",
    "print(\"finish from_pretrained model\")\n",
    "\n",
    "processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n",
    "print(\"finish from_pretrained processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=2,                              \n",
    "    lora_alpha=8,                    \n",
    "    lora_dropout=0.2,                 \n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "print(\"LoRA config created\")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"LoRA applied to model\")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"model moved to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blip_estimetion(url):\n",
    "\n",
    "  image_data = base64.b64decode(url)\n",
    "  image = Image.open(BytesIO(image_data)).convert(\"RGB\")\n",
    "    \n",
    "  prompt = f\"Generate a caption for the provided image. If the image contains any nonsensical or uncommon elements, make sure to highlight them.\"\n",
    "\n",
    "  # Process the image and text together\n",
    "  inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "  outputs = model.generate(\n",
    "          **inputs,\n",
    "          do_sample=False,\n",
    "          num_beams=5,\n",
    "          max_length=150,\n",
    "          min_length=5,\n",
    "          top_p=0, #0.9, # the probability of the answer\n",
    "          repetition_penalty=1.5,\n",
    "          length_penalty=0.6, # A value greater than 1.0 encourages longer sequences, while a value less than 1.0 encourages shorter sequences.\n",
    "          temperature=0.5, #1.2,\n",
    "  )\n",
    "    \n",
    "  generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "  return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choose model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_estimetion = gpt4_estimetion\n",
    "# model_name = 'gpt'\n",
    "# or!!!\n",
    "model_estimetion = blip_estimetion\n",
    "model_name = 'blip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VONpfPrkIT8d"
   },
   "source": [
    "### prepare WHOOPS! dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSGd7fh11ff0",
    "outputId": "9725152f-463a-45ab-a5a8-26dbff6f9345"
   },
   "outputs": [],
   "source": [
    "!pip install -q git-lfs\n",
    "!git clone https://huggingface.co/spaces/nlphuji/whoops-explorer-analysis\n",
    "!pip install -q datasets\n",
    "\n",
    "wmtis = load_dataset(\"nlphuji/wmtis-identify\")['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wmtis))\n",
    "# Slice the dataset to exclude the last index\n",
    "wmtis = wmtis.select(range(len(wmtis) - 1))\n",
    "print(len(wmtis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EP7D6uI3IYhw"
   },
   "source": [
    "### Run captions generation task on strange and normal images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "of-NCDt52-RN"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from openai.error import InvalidRequestError\n",
    "\n",
    "model_captions ={'normal_caption':[],'strange_caption':[]}\n",
    "for record in wmtis:\n",
    "  normal_image = record['normal_image']\n",
    "  strange_image = record['strange_image']\n",
    "\n",
    "  # normal image:\n",
    "  buffered = BytesIO()\n",
    "  normal_image.save(buffered, format=\"PNG\")\n",
    "  normal_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "  try:\n",
    "    normal_caption = model_estimetion(normal_str)\n",
    "    print(normal_caption)\n",
    "    model_captions['normal_caption'].append(normal_caption)\n",
    "  except InvalidRequestError as e:\n",
    "    model_captions['normal_caption'].append(f'error: {normal_caption}')\n",
    "    print(f\"Failed to get caption: {e}\")\n",
    "\n",
    "  # strange image:\n",
    "  buffered = BytesIO()\n",
    "  strange_image.save(buffered, format=\"PNG\")\n",
    "  strange_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "  try:\n",
    "    strange_caption = model_estimetion(strange_str)\n",
    "    print(strange_caption)\n",
    "    model_captions['strange_caption'].append(strange_caption)\n",
    "  except InvalidRequestError as e:\n",
    "    model_captions['strange_caption'].append(f'error: {strange_caption}')\n",
    "    print(f\"Failed to get caption: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4OXFVJYaRC9"
   },
   "outputs": [],
   "source": [
    "# save the outputs to a csv files\n",
    "generated_df = pd.DataFrame({\n",
    "    'strange': model_captions['strange_caption'],\n",
    "    'normal': model_captions['normal_caption']\n",
    "})\n",
    "\n",
    "generated_df.to_csv(f'{model_name}-generated_captions.csv', index=False)  # index=False to avoid writing row numbers"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "blip2",
   "language": "python",
   "name": "blip2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
