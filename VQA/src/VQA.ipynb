{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfTLXP4vz2wf"
      },
      "source": [
        "### Installations and imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8n5jpI9E3116",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06985e1-f24c-4489-ae7b-d489be91e09c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'whoops-explorer-analysis'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 26 (delta 6), reused 6 (delta 6), pack-reused 19 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (26/26), 5.40 KiB | 502.00 KiB/s, done.\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade typing-extensions\n",
        "!pip install -q openai==0.28\n",
        "!pip install -q git-lfs\n",
        "!git clone https://huggingface.co/spaces/nlphuji/whoops-explorer-analysis\n",
        "!pip install -q datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EwzCE4g1AJD"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import os\n",
        "import random\n",
        "import openai\n",
        "import time\n",
        "import ast\n",
        "import pandas as pd\n",
        "import re\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import numpy as np\n",
        "import traceback\n",
        "from openai.error import InvalidRequestError\n",
        "\n",
        "wmtis = load_dataset(\"nlphuji/wmtis-identify\")['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmJdDD8Gp1xf"
      },
      "source": [
        "### Openai and helper functions for using api:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQP15g2ozsGM"
      },
      "outputs": [],
      "source": [
        "def _ms_since_epoch():\n",
        "    return time.perf_counter_ns() // 1000000\n",
        "\n",
        "\n",
        "def set_openai_parameters(engine, max_tokens):\n",
        "    # openai API setup and parameters\n",
        "    openai.api_key = \"key\"\n",
        "    parameters = {\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"top_p\": 0,  # greedy\n",
        "        \"temperature\": 0.5,\n",
        "        \"logprobs\": 5,  # maximal value accorrding to https://beta.openai.com/docs/api-reference/completions/create#completions/create-logprobs, used to be 10...\n",
        "        \"engine\": engine,\n",
        "    }\n",
        "    time_of_last_api_call = _ms_since_epoch()\n",
        "\n",
        "    return parameters, time_of_last_api_call\n",
        "\n",
        "\n",
        "def wait_between_predictions(time_of_last_api_call, min_ms_between_api_calls):\n",
        "    if (\n",
        "        cur_time := _ms_since_epoch()\n",
        "    ) <= time_of_last_api_call + min_ms_between_api_calls:\n",
        "        ms_to_sleep = min_ms_between_api_calls - (cur_time - time_of_last_api_call)\n",
        "        time.sleep(ms_to_sleep / 1000)\n",
        "    time_of_last_api_call = _ms_since_epoch()\n",
        "\n",
        "\n",
        "def predict_sample_openai_gpt(\n",
        "    example,\n",
        "    prompt,\n",
        "    min_ms_between_api_calls: int = 500,\n",
        "    engine: str = \"text-davinci-003\",\n",
        "    max_tokens: int = 100,\n",
        "):\n",
        "    parameters, time_of_last_api_call = set_openai_parameters(engine, max_tokens)\n",
        "    parameters[\"prompt\"] = prompt\n",
        "\n",
        "    # OpenAI limits us to 3000 calls per minute:\n",
        "    # https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\n",
        "    # that is why the default value of min_ms_between_api_calls is 20\n",
        "    wait_between_predictions(time_of_last_api_call, min_ms_between_api_calls)\n",
        "\n",
        "    response = openai.Completion.create(**parameters)\n",
        "\n",
        "    if response is None:\n",
        "        raise Exception(\"Response from OpenAI API is None.\")\n",
        "\n",
        "    # build output data\n",
        "    prediction = dict()\n",
        "    prediction[\"input\"] = prompt\n",
        "    prediction[\"prediction\"] = response.choices[0].text.strip().strip(\".\")  # type:ignore\n",
        "\n",
        "    # build output metadata\n",
        "    metadata = example.copy()  # dict()\n",
        "    metadata[\"logprobs\"] = response.choices[0][\"logprobs\"]  # type:ignore\n",
        "    # \"finish_reason\" is located in a slightly different location in opt\n",
        "    if \"opt\" in engine:\n",
        "        finish_reason = response.choices[0][\"logprobs\"][  # type:ignore\n",
        "            \"finish_reason\"\n",
        "        ]\n",
        "    else:\n",
        "        finish_reason = response.choices[0][\"finish_reason\"]  # type:ignore\n",
        "    metadata[\"finish_reason\"] = finish_reason\n",
        "    if \"opt\" not in engine:\n",
        "        # From the OpenAI API documentation it's not clear what \"index\" is, but let's keep it as well\n",
        "        metadata[\"index\"] = response.choices[0][\"index\"]  # type:ignore\n",
        "\n",
        "    prediction[\"metadata\"] = metadata\n",
        "\n",
        "    return prediction\n",
        "\n",
        "def predict_sample_openai_chatgpt(\n",
        "    prompt,\n",
        "    img_url,\n",
        "    min_ms_between_api_calls: int = 10000,\n",
        "    engine: str = \"gpt-4o\",\n",
        "    max_tokens: int = 100,\n",
        "):\n",
        "    parameters, time_of_last_api_call = set_openai_parameters(engine, max_tokens)\n",
        "    parameters[\"prompt\"] = prompt\n",
        "\n",
        "    # OpenAI limits us to 3000 calls per minute:\n",
        "    # https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\n",
        "    wait_time = 5\n",
        "    time.sleep(wait_time)\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(model=engine, messages=[{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\":prompt},{\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_url}\"\n",
        "}}]}], temperature=parameters['temperature'], top_p=parameters['top_p'])\n",
        "    except openai.error.RateLimitError as e:\n",
        "        wait_time = 10\n",
        "        print(f\"Rate limit reached. Waiting {wait_time} seconds.\")\n",
        "        time.sleep(wait_time)\n",
        "\n",
        "        response = openai.ChatCompletion.create(model=engine, messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                                                temperature=parameters['temperature'], top_p=parameters['top_p'])\n",
        "\n",
        "    if response is None:\n",
        "        raise Exception(\"Response from OpenAI API is None.\")\n",
        "\n",
        "    # build output data\n",
        "    prediction = dict()\n",
        "    prediction[\"input\"] = prompt\n",
        "    prediction[\"prediction\"] = response.choices[0].message['content']  # type:ignore\n",
        "\n",
        "    return prediction\n",
        "\n",
        "def gpt4_estimetion(url,question):\n",
        "  prompt = \"\"\n",
        "  #prompt with explanation\n",
        "  if is_explain == True:\n",
        "      prompt = f\"I provided you an image and a question. Provide a basic description of the answer to the question: {question}, in 1-3 words. You can answer 'no answer' if there is absolutely no answer. Add an explanation either way.\"\n",
        "\n",
        "  #prompt without explanation\n",
        "  else:\n",
        "      prompt = f\"I provided you an image and a question, provide a basic description of the answer to the question: {question}, in 1-3 words. you can answer 'no answer' if there is absolutely no answer visible in the image or you're uncertain\"\n",
        "\n",
        "  gpt4_prediction = predict_sample_openai_chatgpt(prompt,url)\n",
        "  return gpt4_prediction['prediction']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kFcfRJGlf2x"
      },
      "source": [
        "**sanity check:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j66Qak4z1pSK"
      },
      "outputs": [],
      "source": [
        "print(len(wmtis))\n",
        "print(wmtis[45])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emMFgf2NoENs"
      },
      "source": [
        "## loading file and extracting data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-sNZ_fZ1AJK"
      },
      "source": [
        "### Choose type of questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WVOt-do91AJL"
      },
      "outputs": [],
      "source": [
        "# (normal question=question that is reasonable to ask, strange question=question with world knowledge contradiction)\n",
        "type_question = \"normal\" #@param [\"normal\", \"strange\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJsSddRMqUa9"
      },
      "outputs": [],
      "source": [
        "if type_question == \"strange\":\n",
        "    df = pd.read_csv(\"fixedLabels_full.csv\")\n",
        "    questions = []\n",
        "\n",
        "    def extract_embedded_question(full):\n",
        "        try:\n",
        "            # Convert string representation of list to actual list\n",
        "            full_list = ast.literal_eval(full)\n",
        "            if isinstance(full_list, list) and len(full_list) > 1:\n",
        "                return full_list[0]\n",
        "            else:\n",
        "                raise ValueError(\"Question does not have the expected format.\")\n",
        "        except (ValueError, SyntaxError) as e:\n",
        "            print(f\"Error parsing question: {full} - {e}\")\n",
        "            return None\n",
        "\n",
        "    # Apply function to extract the embedded answer\n",
        "    questions = df['full'].apply(extract_embedded_question)\n",
        "\n",
        "\n",
        "elif type_question == \"normal\":\n",
        "    df = pd.read_csv('normal question.csv')\n",
        "    questions = df['question'].tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sanity check:**"
      ],
      "metadata": {
        "id": "o-TbW0pv1q1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(questions)):\n",
        "    print(questions[i])\n",
        "    if i==10:\n",
        "        break\n",
        "print(len(questions))"
      ],
      "metadata": {
        "id": "uhOTZewG1pu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDaXEaHo1AJM"
      },
      "source": [
        "### Choose prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etcAmEtw1AJM"
      },
      "outputs": [],
      "source": [
        "type_prompt = \"blip\" #@param [\"gpt\",\"blip\"] (use the same prompt as in GPT experiment / new one that works better for blip)\n",
        "is_explain = True # #@param [\"True\",\"False\"]  (with explanation or without)\n",
        "explain = \"explain\" if is_explain else \"no explain\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyKS_4lM1AJN"
      },
      "source": [
        "## Blip Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B4WgvCbrPGh",
        "outputId": "29c68a98-2e6b-48ed-c79d-1dd0ca4c1a42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [02:12<00:00, 33.07s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finish from_pretrained model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finish from_pretrained processor\n"
          ]
        }
      ],
      "source": [
        "model = InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-vicuna-7b\", torch_dtype=torch.float16)\n",
        "print(\"finish from_pretrained model\")\n",
        "\n",
        "processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n",
        "print(\"finish from_pretrained processor\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw4cgY341AJO",
        "outputId": "6cc7d610-0278-443a-e5dc-94351c94da4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt-Sij6c1AJO",
        "outputId": "e19564a0-a5ff-4b12-e8af-775e174894c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA config created\n",
            "LoRA applied to model\n",
            "model moved to cuda\n"
          ]
        }
      ],
      "source": [
        "# Define LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=2,\n",
        "    lora_alpha=8,\n",
        "    lora_dropout=0.2,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "\n",
        "print(\"LoRA config created\")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"LoRA applied to model\")\n",
        "\n",
        "model.to(device)\n",
        "print(f\"model moved to {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5UyCQ8vrT_u"
      },
      "outputs": [],
      "source": [
        "def blip_estimetion(url,question):\n",
        "\n",
        "  image_data = base64.b64decode(url)\n",
        "  image = Image.open(BytesIO(image_data)).convert(\"RGB\")\n",
        "\n",
        "  # GPT prompt:\n",
        "  if type_prompt == \"gpt\":\n",
        "      if is_explain:\n",
        "          prompt = f\"I provided you an image and a question. Provide a basic description of the answer to the question: {question}, in 1-3 words. You can answer 'no answer' if there is absolutely no answer. Add an explanation either way.\"\n",
        "      else:\n",
        "            prompt = f\"I provided you an image and a question, provide a basic description of the answer to the question: {question}, in 1-3 words. you can answer 'no answer' if there is absolutely no answer visible in the image or you're uncertain\"\n",
        "\n",
        "  # Best prompt:\n",
        "  elif type_prompt == \"blip\":\n",
        "      if is_explain:\n",
        "        # with explanation:\n",
        "          prompt = f\"Given this image, answer the question: {question} in 1-3 words. Add an explanation.\"\n",
        "      else:\n",
        "        # no explanation:\n",
        "            prompt = f\"Given this image, answer the question: {question} in 1-3 words.\"\n",
        "\n",
        "  # Process the image and text together\n",
        "  inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  outputs = model.generate(\n",
        "          **inputs,\n",
        "          do_sample=False,\n",
        "          num_beams=5,\n",
        "          max_length=200,\n",
        "          min_length=10,\n",
        "          top_p=0.9, # the probability of the answer\n",
        "          repetition_penalty=1.5,\n",
        "          length_penalty=0.6, # A value greater than 1.0 encourages longer sequences, while a value less than 1.0 encourages shorter sequences.\n",
        "          temperature=1.2,\n",
        "  )\n",
        "\n",
        "  generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "  return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5MsL8WAuKnM"
      },
      "source": [
        "## Choose model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IiPbNtgkJ25"
      },
      "outputs": [],
      "source": [
        "# model_estimetion = gpt4_estimetion\n",
        "# model_name = 'gpt'\n",
        "# or!!!\n",
        "model_estimetion = blip_estimetion\n",
        "model_name = 'blip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3FealtW0Bc-"
      },
      "source": [
        "## Generating answers according to the prompt (with weird and normal pictures):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of-NCDt52-RN"
      },
      "outputs": [],
      "source": [
        "# Initialize dictionaries to store questions and corresponding model answers\n",
        "questions_dict = {'normal_question': [], 'strange_question': []}\n",
        "model_answers = {'normal_answer': [], 'strange_answer': []}\n",
        "\n",
        "# Initialize a counter for the question index\n",
        "question_index = -1\n",
        "\n",
        "# Iterate over the records in the dataset\n",
        "for record in wmtis:\n",
        "    question_index += 1\n",
        "\n",
        "    if question_index == len(questions):\n",
        "        break\n",
        "\n",
        "    # Get the normal and strange images from the current record\n",
        "    normal_image = record['normal_image']\n",
        "    strange_image = record['strange_image']\n",
        "\n",
        "    # Convert the normal image to a base64-encoded string (URL format)\n",
        "    buffered = BytesIO()\n",
        "    normal_image.save(buffered, format=\"PNG\")\n",
        "    normal_url = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "    # Try to generate an answer for the normal image using the model\n",
        "    try:\n",
        "        normal_answer = model_estimetion(normal_url, questions[question_index])\n",
        "        model_answers['normal_answer'].append(normal_answer)  # Store the answer\n",
        "        questions_dict['normal_question'].append(questions[question_index])  # Store the corresponding question\n",
        "\n",
        "    except InvalidRequestError as e:\n",
        "        print(f\"Failed to get caption: {e}\")  # Handle any errors during model inference\n",
        "\n",
        "    # Convert the strange image to a base64-encoded string (URL format)\n",
        "    buffered = BytesIO()\n",
        "    strange_image.save(buffered, format=\"PNG\")\n",
        "    strange_url = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "    # Try to generate an answer for the strange image using the model\n",
        "    try:\n",
        "        strange_answer = model_estimetion(strange_url, questions[question_index])\n",
        "\n",
        "        model_answers['strange_answer'].append(strange_answer)  # Store the answer\n",
        "        questions_dict['strange_question'].append(questions[question_index])  # Store the corresponding question\n",
        "\n",
        "    except InvalidRequestError as e:\n",
        "        print(f\"Failed to get caption: {e}\")  # Handle any errors during model inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEMHqQsz0VTu"
      },
      "source": [
        "**Sanity check:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oTinv6QsPl1",
        "outputId": "831754ef-ef0a-4905-fef9-afcd7628a167"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "102\n"
          ]
        }
      ],
      "source": [
        "print(len(model_answers['strange_answer']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK7HIXOQqxqc"
      },
      "source": [
        "## Saving result in csv files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4OXFVJYaRC9"
      },
      "outputs": [],
      "source": [
        "normal_df = pd.DataFrame({\n",
        "    'question': questions_dict['normal_question'],\n",
        "    'answer': model_answers['normal_answer']\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "normal_df.to_csv(f'{type_question}_questions_normal_answers_{type_prompt}_prompt_{explain}_{model_name}.csv', index=False)\n",
        "\n",
        "\n",
        "strange_df = pd.DataFrame({\n",
        "    'question': questions_dict['strange_question'],\n",
        "    'answer': model_answers['strange_answer']\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "strange_df.to_csv(f'{type_question}_questions_strange_answers_{type_prompt}_prompt_{explain}_{model_name}.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "blip2",
      "language": "python",
      "name": "blip2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}